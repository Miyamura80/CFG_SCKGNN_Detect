{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "NOdT7ouc9Azc"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVr_t21wuTm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfdf0b7-7189-48e2-dc82-7c658877a344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.1/448.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q dm-haiku spektral optax neptune flax torch torch-geometric jraph wget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U jax jaxlib\n"
      ],
      "metadata": {
        "id": "38UXTtnbG_Lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addcc135-fcb1-4bc1-8821-084f00e26aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atl08i3CIhsD",
        "outputId": "8dd5532e-726b-41e2-81fe-1e50db685901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda11_cudnn82] in /usr/local/lib/python3.10/dist-packages (0.4.10)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (0.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (1.10.1)\n",
            "INFO: pip is looking at multiple versions of jax[cuda11-cudnn82] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax[cuda11_cudnn82]\n",
            "  Using cached jax-0.4.10-py3-none-any.whl\n",
            "  Downloading jax-0.4.9.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxlib==0.4.7+cuda11.cudnn82 (from jax[cuda11_cudnn82])\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.7%2Bcuda11.cudnn82-cp310-cp310-manylinux2014_x86_64.whl (149.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=70d4f49c024c30a924c7dc9887621e27abf526f3ba119db50996a95f01854e54\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/6f/35/a8fac8b61de8e0d9eb07988481528898561923e260b1fa7d2f\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.10\n",
            "    Uninstalling jaxlib-0.4.10:\n",
            "      Successfully uninstalled jaxlib-0.4.10\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.10\n",
            "    Uninstalling jax-0.4.10:\n",
            "      Successfully uninstalled jax-0.4.10\n",
            "Successfully installed jax-0.4.8 jaxlib-0.4.7+cuda11.cudnn82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init\n"
      ],
      "metadata": {
        "id": "xiSE_YeOuzz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import neptune\n",
        "import statistics\n",
        "import itertools\n",
        "import pickle\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "import jraph\n",
        "import haiku as hk\n",
        "import functools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from typing import Sequence\n",
        "from spektral.utils import normalized_adjacency\n",
        "\n",
        "\n",
        "# Torch Dataset related stuff\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import ZINC\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.loader import DataLoader\n"
      ],
      "metadata": {
        "id": "5pQ6LjDbuuRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KGNN Models\n"
      ],
      "metadata": {
        "id": "t-GC7q2Ouvzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GIN"
      ],
      "metadata": {
        "id": "YW1Ou_icuw-0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXuM72CrJ2u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "6j9_JPcxzv2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_model(config: dict, hidden_channels: List[int]) -> Model, Params"
      ],
      "metadata": {
        "id": "Rrs5iqsazw_k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClS5OfCAz29S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76510475-5b66-4fa6-d547-5bdea8f17f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None"
      ],
      "metadata": {
        "id": "NOdT7ouc9Azc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_jraph_to_networkx_graph(jraph_graph: jraph.GraphsTuple) -> nx.Graph:\n",
        "  nodes, edges, receivers, senders, _, _, _ = jraph_graph\n",
        "  nx_graph = nx.DiGraph()\n",
        "  if nodes is None:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n)\n",
        "  else:\n",
        "    for n in range(jraph_graph.n_node[0]):\n",
        "      nx_graph.add_node(n, node_feature=nodes[n])\n",
        "  if edges is None:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(int(senders[e]), int(receivers[e]))\n",
        "  else:\n",
        "    for e in range(jraph_graph.n_edge[0]):\n",
        "      nx_graph.add_edge(\n",
        "          int(senders[e]), int(receivers[e]), edge_feature=edges[e])\n",
        "  return nx_graph\n",
        "\n",
        "def draw_jraph_graph_structure(jraph_graph: jraph.GraphsTuple) -> None:\n",
        "  nx_graph = convert_jraph_to_networkx_graph(jraph_graph)\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(\n",
        "      nx_graph, pos=pos, with_labels=True, node_size=500, font_color='yellow')"
      ],
      "metadata": {
        "id": "u4cgrZ5S83me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "-DioGOJe0pVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Standard Dataset: 70/20/10 Split\n",
        "X_train, y_train, adj_train"
      ],
      "metadata": {
        "id": "t_ORq3ao0s4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download jraph version of MUTAG.\n",
        "# !wget -P /tmp/ https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle\n",
        "# with open('/tmp/mutag.pickle', 'rb') as f:\n",
        "#   dataset = pickle.load(f)\n",
        "# len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCH5yZjZ8Toh",
        "outputId": "903ea169-76f6-40e5-ffdb-3b52394345be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-13 09:50:57--  https://storage.googleapis.com/dm-educational/assets/graph-nets/jraph_datasets/mutag.pickle\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 352981 (345K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/mutag.pickle’\n",
            "\n",
            "mutag.pickle        100%[===================>] 344.71K  1.29MB/s    in 0.3s    \n",
            "\n",
            "2023-05-13 09:50:58 (1.29 MB/s) - ‘/tmp/mutag.pickle’ saved [352981/352981]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "188"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import urllib.request\n",
        "import random\n",
        "\n",
        "vuln_list = [\"delegatecall\", \"integeroverflow\", \"reentrancy\", \"timestamp\"]\n",
        "\n",
        "vuln_dataset = []\n",
        "\n",
        "for vuln in vuln_list:\n",
        "  url = f\"https://github.com/Miyamura80/CFG_SCKGNN_Detect/raw/main/dataset/{vuln}_fixed.pkl\"\n",
        "  file_path = f\"/tmp/{vuln}_fixed.pkl\"\n",
        "  urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "  # Load the pickle file\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      dataset_1 = pickle.load(f)\n",
        "  vuln_dataset.append(dataset_1)\n",
        "\n",
        "print(len(vuln_dataset))\n"
      ],
      "metadata": {
        "id": "EDR25HilrMQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vuln_dataset[0][0][\"target\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_IZMQ4Jvsly",
        "outputId": "182e3831-2787-4764-e5df-597fb5d87a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = []\n",
        "for i,ds in enumerate(vuln_dataset):\n",
        "  main_ds = [d for d in ds]\n",
        "  for j,ds_other in enumerate(vuln_dataset):\n",
        "    if i!=j:\n",
        "        for g in ds_other:\n",
        "          graph = g[\"input_graph\"]\n",
        "          main_ds.append({\"target\": [0], \"input_graph\": graph})\n",
        "  random.shuffle(main_ds)\n",
        "  datasets.append(main_ds)"
      ],
      "metadata": {
        "id": "YI2toMe_sLyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_jraph_graph_structure(datasets[0][1][\"input_graph\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "xMe4FAZG9PQf",
        "outputId": "ed6a41b2-9266-4a37-adbf-eb442e4e6a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets[3]\n",
        "dataset = [g for g in dataset if g['input_graph'].edges.shape[0] > 0]\n",
        "\n",
        "print(f\"Target: {dataset[104]['target']}\")\n",
        "\n",
        "sum = 0\n",
        "for g in dataset:\n",
        "  sum += g[\"target\"][0]\n",
        "print(sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8oZNixY9PXb",
        "outputId": "30cbe9ad-f1f8-4187-850a-dcf85d374956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: [0]\n",
            "349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ratios\n",
        "train_ratio = 0.7\n",
        "valid_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Calculate the number of samples for each set\n",
        "n_samples = len(dataset)\n",
        "n_train = int(train_ratio * n_samples)\n",
        "n_valid = int(valid_ratio * n_samples)\n",
        "\n",
        "train_ds = dataset[:n_train]\n",
        "valid_ds = dataset[n_train:(n_train+n_valid)]\n",
        "test_ds = dataset[(n_train+n_valid):]\n",
        "print(n_train, n_valid)"
      ],
      "metadata": {
        "id": "nS8cRkiX9qTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a112d7ae-2817-4ce2-b104-517680e850bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "762 217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "-d45ZXy21JXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## padded_batch, labels, num_nodes, pad_nodes_to = create_padded_batch(train_ds)"
      ],
      "metadata": {
        "id": "JOiTk0930tfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padded_batch(ds):\n",
        "  # for g in ds:\n",
        "  #   print(g[\"input_graph\"].edges.shape)\n",
        "  ds_nodes_max = max([g[\"input_graph\"].nodes.shape[0] for g in ds])\n",
        "  ds_edges_max = max([g[\"input_graph\"].edges.shape[0] for g in ds])\n",
        "\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(ds_nodes_max))\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(ds_edges_max))\n",
        "  print(jnp.sum(ds_edges_max))\n",
        "\n",
        "  # padded_ds = [pad_graph_to_nearest_power_of_two(g[\"input_graph\"]) for g in ds]\n",
        "  padded_ds = [jraph.pad_with_graphs(g[\"input_graph\"], pad_nodes_to, pad_edges_to) for g in ds]\n",
        "  labels = jnp.array([g[\"target\"] for g in ds])\n",
        "  num_nodes = jnp.array([g[\"input_graph\"].n_node for g in ds])\n",
        "  return padded_ds, labels, num_nodes, pad_nodes_to\n",
        "\n",
        "padded_batch, labels, num_nodes, pad_nodes_to = create_padded_batch(train_ds)\n",
        "print([g.edges.shape for g in padded_batch])"
      ],
      "metadata": {
        "id": "YgEn9usjoRQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61746dc2-cb49-462b-c05c-7d8be43a51fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n",
            "[(128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4), (128, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "h0GfnzNd1F05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "k5AY-tqwMTbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Model"
      ],
      "metadata": {
        "id": "TySLirZEAJzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN:\n",
        "    def __init__(self, hidden_dim, n_layers, n_classes=2):\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def update_node_fn(self, node, agg_send, agg_rec, globals_):\n",
        "        node_update = jnp.concatenate([node, agg_send, agg_rec], axis=-1)\n",
        "        net = hk.Sequential([hk.Linear(self.hidden_dim), jax.nn.relu,hk.Linear(self.hidden_dim)])\n",
        "        return net(node_update)\n",
        "\n",
        "    def update_edge_fn(self, edge, sender_node, receiver_node, globals_):\n",
        "        edge_update = jnp.concatenate([edge, sender_node], axis=-1)\n",
        "        # node_update = jnp.concatenate([node, agg_send, agg_rec], axis=-1)\n",
        "        net = hk.Sequential([hk.Linear(self.hidden_dim), jax.nn.relu,hk.Linear(self.hidden_dim)])\n",
        "        return net(edge_update)\n",
        "        # return net(edge)\n",
        "\n",
        "    def classifier_head(self, node, agg_send, agg_rec, globals_):\n",
        "        node_update = jnp.concatenate([node, agg_send, agg_rec], axis=-1)\n",
        "        net = hk.Sequential([hk.Linear(self.hidden_dim), jax.nn.relu,hk.Linear(self.n_classes)])\n",
        "        return net(node_update)\n",
        "\n",
        "\n",
        "    def build_layer(self, layer_num):\n",
        "        if layer_num==self.n_layers-1:\n",
        "            return jraph.GraphNetwork(\n",
        "              update_node_fn=self.update_node_fn,\n",
        "              update_edge_fn=self.update_edge_fn,\n",
        "            )\n",
        "        else:\n",
        "            return jraph.GraphNetwork(\n",
        "              update_node_fn=self.classifier_head,\n",
        "              update_edge_fn=self.update_edge_fn,\n",
        "            )\n",
        "\n",
        "    def net_fn(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "        graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "        embedder = jraph.GraphMapFeatures(hk.Linear(self.hidden_dim), hk.Linear(self.hidden_dim), hk.Linear(self.hidden_dim))\n",
        "        layers = [self.build_layer(layer_num) for layer_num in range(self.n_layers-1)]\n",
        "        out_graph = embedder(graph)\n",
        "        for layer in layers:\n",
        "          out_graph = layer(out_graph)\n",
        "        return out_graph\n",
        "\n",
        "\n",
        "\n",
        "gin_model = GNN(64, 2)\n",
        "\n",
        "graph_1 = train_ds[0][\"input_graph\"]\n",
        "net_fn_partial = functools.partial(gin_model.net_fn, graph=graph_1)\n",
        "net = hk.without_apply_rng(hk.transform(net_fn_partial))\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = net.init(rng)\n",
        "pred_graph = net.apply(params)\n",
        "print(pred_graph.nodes.shape)\n",
        "\n",
        "# @jraph.concatenated_args\n",
        "# def update_global_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "#   \"\"\"Global update function for graph net.\"\"\"\n",
        "#   # MUTAG is a binary classification task, so output pos neg logits.\n",
        "#   net = hk.Sequential([hk.Linear(FEAT), jax.nn.relu, hk.Linear(2)])\n",
        "#   return net(feats)\n"
      ],
      "metadata": {
        "id": "4ujaVA9KALtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2930ee-7797-4252-ba29-8791a4f39370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "AsgyODabSEP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New loss"
      ],
      "metadata": {
        "id": "8jh9XDiot4Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dataset_loss(batched_graph: jraph.GraphsTuple, labels: jnp.ndarray,\n",
        "                         params: hk.Params, net: hk.Transformed, n_graphs: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "\n",
        "    updated_node_features = net.apply(params, batched_graph)\n",
        "    feature_num = updated_node_features.nodes.shape[1]\n",
        "    reshaped_features = updated_node_features.nodes.reshape((-1, pad_nodes_to, feature_num))\n",
        "\n",
        "    summed_features = jnp.sum(reshaped_features, axis=1)\n",
        "    preds = jax.nn.log_softmax(summed_features, axis=-1)\n",
        "\n",
        "    targets = jax.nn.one_hot(labels.flatten(), num_classes=2)\n",
        "    loss = -jnp.sum(targets * preds) / n_graphs\n",
        "    acc = jnp.mean(jnp.equal(jnp.argmax(preds, axis=1), jnp.argmax(targets, axis=1)))\n",
        "\n",
        "    return jnp.array(loss), jnp.array(acc)\n"
      ],
      "metadata": {
        "id": "o58rnOJAqhTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Junk\n"
      ],
      "metadata": {
        "id": "ELsAitwwSCy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gin_model = GNN(64, 2)\n",
        "\n",
        "# Creating batched dataset\n",
        "padded_batch, labels, num_nodes, pad_nodes_to = create_padded_batch(train_ds)\n",
        "batched_graph = jraph.batch(padded_batch)\n",
        "\n",
        "\n",
        "# Definitining haiku for training purpose\n",
        "# net_fn_partial = functools.partial(gin_model.net_fn, graph=batched_graph)\n",
        "# net = hk.without_apply_rng(hk.transform(net_fn_partial))\n",
        "net = hk.without_apply_rng(hk.transform(gin_model.net_fn))\n",
        "\n",
        "\n",
        "# GNN initialization + test inference\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = net.init(rng, batched_graph)\n",
        "updated_node_features = net.apply(params, batched_graph)\n",
        "print(\"updated_node_features\", updated_node_features.nodes.shape)\n",
        "\n",
        "\n",
        "compute_loss_fn = functools.partial(compute_dataset_loss,\n",
        "                                    batched_graph=batched_graph,\n",
        "                                    net=net,\n",
        "                                    labels=labels,\n",
        "                                    n_graphs=len(train_ds))\n",
        "\n",
        "\n",
        "\n",
        "def wrapper_fn(params):\n",
        "    return compute_loss_fn(params=params)\n",
        "\n",
        "wrapper_fn = jax.jit(jax.value_and_grad(wrapper_fn, has_aux=True))\n",
        "(loss, acc), grad = wrapper_fn(params)\n",
        "\n",
        "print(loss)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrOsGfFjBSK9",
        "outputId": "24944810-b9c2-4ec5-e7ac-e2c058671c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated_node_features (4192, 2)\n",
            "0.8404377\n",
            "0.35114503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPip-CIhFYIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train, Evaluate"
      ],
      "metadata": {
        "id": "ZfiAvXFWRTMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(params, opt_state, compute_loss_fn, opt_update):\n",
        "    (loss, acc), grad = compute_loss_fn(params)\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss, acc\n"
      ],
      "metadata": {
        "id": "8zZDZNKiS0Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train_and_eval"
      ],
      "metadata": {
        "id": "v0kNwJf7eV4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(config: dict, dataset: List[Dict[str, Any]], val_ds: List[Dict[str, Any]], neptune_client=None) -> hk.Params:\n",
        "\n",
        "  # Creating batched dataset\n",
        "  padded_batch, labels, num_nodes, pad_nodes_to = create_padded_batch(dataset)\n",
        "  batched_train = jraph.batch(padded_batch)\n",
        "\n",
        "  val_padded_batch, val_labels, val_num_nodes, val_pad_nodes_to = create_padded_batch(val_ds)\n",
        "  batched_val = jraph.batch(val_padded_batch)\n",
        "\n",
        "  # Initializing model\n",
        "  model = GNN(config[\"hidden_channels\"], config[\"num_layers\"])\n",
        "  net = hk.without_apply_rng(hk.transform(model.net_fn))\n",
        "  rng_seed = 0\n",
        "  neptune_client[\"params\"][\"rng_seed\"] = rng_seed\n",
        "  rng = jax.random.PRNGKey(rng_seed)\n",
        "  rng, current_rng = jax.random.split(rng)\n",
        "  params = net.init(rng, batched_train)\n",
        "\n",
        "  # Early stopping measures\n",
        "  best_acc_this_config = 0.0\n",
        "  best_train_tracker = []\n",
        "  best_state_this_config = None\n",
        "\n",
        "  # Loss function\n",
        "  compute_loss_fn = functools.partial(compute_dataset_loss,\n",
        "                                    batched_graph=batched_train,\n",
        "                                    net=net,\n",
        "                                    labels=labels,\n",
        "                                    n_graphs=len(dataset))\n",
        "  def wrapper_fn(params):\n",
        "    return compute_loss_fn(params=params)\n",
        "  # jit the loss function for single XLA, jax.pmap for multi-XLA\n",
        "  wrapper_fn = jax.jit(jax.value_and_grad(wrapper_fn, has_aux=True))\n",
        "  (loss, acc), grad = wrapper_fn(params)\n",
        "\n",
        "\n",
        "  # Validation loss function\n",
        "  valid_loss_fn = functools.partial(compute_dataset_loss,\n",
        "                                    batched_graph=batched_val,\n",
        "                                    net=net,\n",
        "                                    labels=val_labels,\n",
        "                                    n_graphs=len(val_ds))\n",
        "  def wrapper_fn_2(params):\n",
        "    return valid_loss_fn(params=params)\n",
        "  # jit the loss function for single XLA, jax.pmap for multi-XLA\n",
        "  evaluate = jax.jit(wrapper_fn_2)\n",
        "\n",
        "\n",
        "  for try_n in range(config[\"retry_num\"]):\n",
        "    # Re-initialize training state\n",
        "    params = net.init(rng, batched_train)\n",
        "    opt_init, opt_update = optax.adam(config[\"lr\"]) if config[\"optimizer\"]==\"Adam\" else optax.sgd(config[\"lr\"], config[\"momentum\"])\n",
        "    opt_state = opt_init(params)\n",
        "    rng, current_rng = jax.random.split(rng)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    epochs_without_improvement = 0\n",
        "    best_test_accuracy = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    # List to store test accuracies\n",
        "    try_train_tracker = []\n",
        "    val_accuracies = []\n",
        "    best_perform = True\n",
        "\n",
        "    for idx in range(1, config[\"epoch\"] + 1):\n",
        "\n",
        "      params, opt_state, train_loss, train_accuracy = train_epoch(params, opt_state, wrapper_fn, opt_update)\n",
        "      val_loss, val_accuracy = evaluate(params)\n",
        "\n",
        "      val_accuracies.append(val_accuracy)\n",
        "      # Update best test accuracy and model state\n",
        "      if val_accuracy > best_test_accuracy:\n",
        "          best_test_accuracy = val_accuracy\n",
        "          best_model_state = params\n",
        "          epochs_without_improvement = 0\n",
        "      else:\n",
        "          epochs_without_improvement += 1\n",
        "\n",
        "      # Early stopping\n",
        "      if epochs_without_improvement >= config[\"early_stopping_threshold\"]:\n",
        "          best_perform = best_test_accuracy > best_acc_this_config\n",
        "          print(f\"Early stopping at epoch {idx}.\")\n",
        "          break\n",
        "\n",
        "      # Log\n",
        "      try_train_tracker.append({\n",
        "          \"train_loss\": train_loss,\n",
        "          \"train_accuracy\": train_accuracy,\n",
        "          \"val_accuracy\": val_accuracy,\n",
        "          \"val_loss\": val_loss,\n",
        "      })\n",
        "\n",
        "      if idx % 20 == 0:\n",
        "          print(\n",
        "              'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, val_loss: %.4f, val_accuracy: %.2f'\n",
        "              % (idx, train_loss, train_accuracy * 100, val_loss,\n",
        "                val_accuracy * 100))\n",
        "    if best_perform:\n",
        "        best_acc_this_config = best_test_accuracy\n",
        "        best_model_this_config = best_model_state\n",
        "        best_train_tracker = try_train_tracker\n",
        "\n",
        "        neptune_client[\"params\"][\"best_val_accuracy\"] = best_acc_this_config\n",
        "        neptune_client[\"params\"][\"stop_epoch\"] = idx\n",
        "        val_acc_floats = [arr.item() for arr in val_accuracies]\n",
        "        neptune_client[\"params\"][\"val_acc_std\"] = statistics.stdev(val_acc_floats)\n",
        "        best_perform = False\n",
        "\n",
        "  # Plot the best run\n",
        "  for i,epoch_detail in enumerate(best_train_tracker):\n",
        "      neptune_client['train/train_loss'].append(epoch_detail[\"train_loss\"])\n",
        "      neptune_client['train/train_accuracy'].append(epoch_detail[\"train_accuracy\"])\n",
        "      neptune_client['train/val_loss'].append(epoch_detail[\"val_loss\"])\n",
        "      neptune_client['train/val_accuracy'].append(epoch_detail[\"val_accuracy\"])\n",
        "\n",
        "  neptune_client.stop()\n",
        "  return best_state_this_config"
      ],
      "metadata": {
        "id": "wpxDYHJJAwMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Loop"
      ],
      "metadata": {
        "id": "5vdfwiUqJtBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_list = [0.001,0.0001]\n",
        "num_layer_list = [2,3]\n",
        "hidden_ch_list = [16,32,64,128]\n",
        "# lr_list = [0.01]\n",
        "# num_layer_list = [2]\n",
        "# hidden_ch_list = [128]\n",
        "\n",
        "prod_params = list(itertools.product(lr_list, num_layer_list,hidden_ch_list))\n",
        "\n",
        "dataset_param_name = \"timestamp\"\n",
        "\n",
        "first_datapoint = dataset[0]\n",
        "edge_feat_dim = first_datapoint[\"input_graph\"].nodes[0].shape[0]\n",
        "node_feat_dim = first_datapoint[\"input_graph\"].nodes[0].shape[0]\n",
        "\n",
        "for (lr, n_layers,hidden_ch) in prod_params:\n",
        "    run = neptune.init_run(\n",
        "        project=\"miyamura80/Slither-Graph-CFG\",\n",
        "        api_token=\"\",\n",
        "        capture_hardware_metrics=True,\n",
        "        capture_stderr=True,\n",
        "        capture_stdout=True,\n",
        "    )\n",
        "\n",
        "    config = {\"lr\": lr,\n",
        "              \"optimizer\": \"SGD\",\n",
        "              \"loss\": \"ce\",\n",
        "              \"epoch\": 300,\n",
        "              \"momentum\": 0.9,\n",
        "              \"early_stopping_threshold\": 100,\n",
        "              \"retry_num\": 5,\n",
        "              # Dataset Related,\n",
        "              \"dataset\": dataset_param_name,\n",
        "              # Model Related\n",
        "              \"model\": \"GIN\",\n",
        "              \"num_layers\": n_layers,\n",
        "              \"hidden_channels\": hidden_ch,\n",
        "              \"in_channels\": node_feat_dim,\n",
        "              \"edge_feat_dim\": edge_feat_dim,\n",
        "              \"out_channels\": 2,\n",
        "              }\n",
        "    run[\"parameters\"] = config\n",
        "    print(\"Parameters: \",(lr, n_layers,hidden_ch))\n",
        "    params = train_and_evaluate(config, train_ds, valid_ds, run)\n",
        "\n",
        "    run.stop()\n",
        "    print(\"finished, submitting to neptune...\")\n",
        "    time.sleep(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH5uHLcrJucd",
        "outputId": "d301511f-7621-4308-bf34-a1e8f33f2b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-203\n",
            "Parameters:  (0.001, 2, 16)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 1.8247, train_accuracy: 67.85, val_loss: 3.6267, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 1.1508, train_accuracy: 67.85, val_loss: 1.9654, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.9662, train_accuracy: 67.85, val_loss: 1.0826, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6410, train_accuracy: 66.80, val_loss: 0.7929, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6241, train_accuracy: 67.32, val_loss: 0.7314, val_accuracy: 64.06\n",
            "epoch: 120, train_loss: 0.6207, train_accuracy: 67.32, val_loss: 0.7105, val_accuracy: 64.06\n",
            "epoch: 140, train_loss: 0.6198, train_accuracy: 67.45, val_loss: 0.6971, val_accuracy: 64.06\n",
            "epoch: 160, train_loss: 0.6192, train_accuracy: 67.45, val_loss: 0.6898, val_accuracy: 64.06\n",
            "Early stopping at epoch 170.\n",
            "epoch: 20, train_loss: 0.6513, train_accuracy: 67.85, val_loss: 0.9690, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6230, train_accuracy: 67.85, val_loss: 0.9619, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6203, train_accuracy: 67.72, val_loss: 0.8550, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6195, train_accuracy: 67.85, val_loss: 0.8582, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6190, train_accuracy: 67.85, val_loss: 0.8834, val_accuracy: 64.06\n",
            "Early stopping at epoch 103.\n",
            "epoch: 20, train_loss: 0.6199, train_accuracy: 67.98, val_loss: 0.6360, val_accuracy: 63.59\n",
            "epoch: 40, train_loss: 0.6174, train_accuracy: 67.85, val_loss: 0.6359, val_accuracy: 66.36\n",
            "epoch: 60, train_loss: 0.6157, train_accuracy: 68.11, val_loss: 0.6349, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6148, train_accuracy: 67.98, val_loss: 0.6344, val_accuracy: 65.44\n",
            "epoch: 100, train_loss: 0.6141, train_accuracy: 67.98, val_loss: 0.6339, val_accuracy: 65.90\n",
            "epoch: 120, train_loss: 0.6134, train_accuracy: 67.98, val_loss: 0.6335, val_accuracy: 65.90\n",
            "Early stopping at epoch 124.\n",
            "epoch: 20, train_loss: 0.6822, train_accuracy: 62.73, val_loss: 0.7939, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6231, train_accuracy: 67.85, val_loss: 1.0135, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6197, train_accuracy: 67.06, val_loss: 0.8681, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6193, train_accuracy: 67.06, val_loss: 0.9012, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6190, train_accuracy: 67.32, val_loss: 0.8787, val_accuracy: 64.06\n",
            "Early stopping at epoch 108.\n",
            "epoch: 20, train_loss: 0.6578, train_accuracy: 67.85, val_loss: 0.8889, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6560, train_accuracy: 66.27, val_loss: 0.6315, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6202, train_accuracy: 67.32, val_loss: 0.6277, val_accuracy: 64.52\n",
            "epoch: 80, train_loss: 0.6136, train_accuracy: 67.98, val_loss: 0.6335, val_accuracy: 64.98\n",
            "epoch: 100, train_loss: 0.6127, train_accuracy: 67.98, val_loss: 0.6304, val_accuracy: 64.98\n",
            "epoch: 120, train_loss: 0.6116, train_accuracy: 67.98, val_loss: 0.6272, val_accuracy: 64.98\n",
            "Early stopping at epoch 133.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 550 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 550 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-203/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-204\n",
            "Parameters:  (0.001, 2, 32)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.7289, train_accuracy: 67.85, val_loss: 0.7342, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6240, train_accuracy: 67.72, val_loss: 0.9941, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6205, train_accuracy: 68.50, val_loss: 0.8792, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6161, train_accuracy: 67.98, val_loss: 0.8511, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6149, train_accuracy: 67.85, val_loss: 0.8668, val_accuracy: 64.06\n",
            "Early stopping at epoch 105.\n",
            "epoch: 20, train_loss: 0.6974, train_accuracy: 67.85, val_loss: 1.0745, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6342, train_accuracy: 67.85, val_loss: 1.0650, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6127, train_accuracy: 66.93, val_loss: 1.2410, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6112, train_accuracy: 67.06, val_loss: 1.3046, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6099, train_accuracy: 66.54, val_loss: 1.3148, val_accuracy: 64.06\n",
            "Early stopping at epoch 109.\n",
            "epoch: 20, train_loss: 0.6383, train_accuracy: 66.67, val_loss: 0.6712, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6213, train_accuracy: 68.11, val_loss: 0.6816, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6174, train_accuracy: 67.85, val_loss: 0.6840, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6160, train_accuracy: 67.98, val_loss: 0.6878, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6150, train_accuracy: 68.11, val_loss: 0.6911, val_accuracy: 64.06\n",
            "Early stopping at epoch 106.\n",
            "epoch: 20, train_loss: 0.6662, train_accuracy: 67.85, val_loss: 0.7548, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6236, train_accuracy: 67.72, val_loss: 0.7577, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6155, train_accuracy: 67.59, val_loss: 0.7496, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6126, train_accuracy: 67.72, val_loss: 0.7555, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6105, train_accuracy: 67.59, val_loss: 0.7585, val_accuracy: 64.06\n",
            "Early stopping at epoch 103.\n",
            "epoch: 20, train_loss: 0.6568, train_accuracy: 67.72, val_loss: 0.7023, val_accuracy: 50.23\n",
            "epoch: 40, train_loss: 0.6377, train_accuracy: 67.72, val_loss: 0.6705, val_accuracy: 64.98\n",
            "epoch: 60, train_loss: 0.6197, train_accuracy: 67.85, val_loss: 0.8605, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6161, train_accuracy: 68.11, val_loss: 0.7003, val_accuracy: 49.77\n",
            "epoch: 100, train_loss: 0.6133, train_accuracy: 67.98, val_loss: 0.7535, val_accuracy: 35.48\n",
            "epoch: 120, train_loss: 0.6120, train_accuracy: 67.85, val_loss: 0.7369, val_accuracy: 36.41\n",
            "epoch: 140, train_loss: 0.6108, train_accuracy: 67.85, val_loss: 0.7499, val_accuracy: 35.48\n",
            "Early stopping at epoch 155.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 632 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 632 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-204/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-205\n",
            "Parameters:  (0.001, 2, 64)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.7103, train_accuracy: 54.46, val_loss: 1.3085, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6230, train_accuracy: 67.59, val_loss: 1.1172, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6170, train_accuracy: 67.59, val_loss: 0.9900, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6119, train_accuracy: 67.59, val_loss: 0.9751, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6096, train_accuracy: 67.06, val_loss: 0.9853, val_accuracy: 64.06\n",
            "Early stopping at epoch 108.\n",
            "epoch: 20, train_loss: 0.6647, train_accuracy: 64.44, val_loss: 0.8888, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6173, train_accuracy: 67.85, val_loss: 0.9666, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6140, train_accuracy: 68.24, val_loss: 0.9197, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6108, train_accuracy: 67.98, val_loss: 0.9250, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6086, train_accuracy: 67.85, val_loss: 0.9345, val_accuracy: 64.06\n",
            "Early stopping at epoch 106.\n",
            "epoch: 20, train_loss: 0.7530, train_accuracy: 67.85, val_loss: 0.7282, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6405, train_accuracy: 66.67, val_loss: 0.7698, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6164, train_accuracy: 67.59, val_loss: 0.7386, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6121, train_accuracy: 67.19, val_loss: 0.7373, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6098, train_accuracy: 67.45, val_loss: 0.7306, val_accuracy: 64.06\n",
            "Early stopping at epoch 117.\n",
            "epoch: 20, train_loss: 0.6310, train_accuracy: 67.85, val_loss: 0.6419, val_accuracy: 67.28\n",
            "epoch: 40, train_loss: 0.6206, train_accuracy: 67.59, val_loss: 0.6349, val_accuracy: 64.52\n",
            "epoch: 60, train_loss: 0.6162, train_accuracy: 68.37, val_loss: 0.6349, val_accuracy: 64.98\n",
            "epoch: 80, train_loss: 0.6132, train_accuracy: 68.11, val_loss: 0.6322, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6114, train_accuracy: 67.98, val_loss: 0.6302, val_accuracy: 63.59\n",
            "Early stopping at epoch 113.\n",
            "epoch: 20, train_loss: 0.6207, train_accuracy: 67.45, val_loss: 0.8220, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6163, train_accuracy: 67.32, val_loss: 0.7320, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6117, train_accuracy: 67.85, val_loss: 0.7057, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6088, train_accuracy: 68.11, val_loss: 0.6935, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6068, train_accuracy: 68.11, val_loss: 0.6877, val_accuracy: 64.52\n",
            "Early stopping at epoch 107.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 478 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 478 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-205/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-206\n",
            "Parameters:  (0.001, 2, 128)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.8547, train_accuracy: 32.15, val_loss: 0.7169, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6304, train_accuracy: 67.98, val_loss: 0.7312, val_accuracy: 39.17\n",
            "epoch: 60, train_loss: 0.6146, train_accuracy: 67.85, val_loss: 0.7330, val_accuracy: 43.32\n",
            "epoch: 80, train_loss: 0.6009, train_accuracy: 67.59, val_loss: 0.7395, val_accuracy: 43.78\n",
            "epoch: 100, train_loss: 0.5979, train_accuracy: 67.98, val_loss: 0.7420, val_accuracy: 47.93\n",
            "Early stopping at epoch 114.\n",
            "epoch: 20, train_loss: 1.1578, train_accuracy: 32.15, val_loss: 1.4702, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.7036, train_accuracy: 67.85, val_loss: 0.7645, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6228, train_accuracy: 67.72, val_loss: 1.0617, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6066, train_accuracy: 67.06, val_loss: 1.0002, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6042, train_accuracy: 67.45, val_loss: 0.9698, val_accuracy: 64.06\n",
            "Early stopping at epoch 107.\n",
            "epoch: 20, train_loss: 0.6654, train_accuracy: 64.83, val_loss: 0.6502, val_accuracy: 66.36\n",
            "epoch: 40, train_loss: 0.6297, train_accuracy: 68.11, val_loss: 0.8731, val_accuracy: 35.48\n",
            "epoch: 60, train_loss: 0.6095, train_accuracy: 67.98, val_loss: 0.6634, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6054, train_accuracy: 67.59, val_loss: 0.6906, val_accuracy: 53.00\n",
            "epoch: 100, train_loss: 0.6030, train_accuracy: 67.59, val_loss: 0.6755, val_accuracy: 56.68\n",
            "epoch: 120, train_loss: 0.6010, train_accuracy: 68.24, val_loss: 0.6780, val_accuracy: 56.22\n",
            "Early stopping at epoch 135.\n",
            "epoch: 20, train_loss: 0.6307, train_accuracy: 67.85, val_loss: 0.6989, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.8687, train_accuracy: 32.15, val_loss: 0.8309, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6085, train_accuracy: 68.11, val_loss: 0.6953, val_accuracy: 64.52\n",
            "epoch: 80, train_loss: 0.6015, train_accuracy: 67.32, val_loss: 0.6436, val_accuracy: 63.59\n",
            "epoch: 100, train_loss: 0.5977, train_accuracy: 67.59, val_loss: 0.6473, val_accuracy: 63.59\n",
            "epoch: 120, train_loss: 0.5957, train_accuracy: 67.72, val_loss: 0.6489, val_accuracy: 63.59\n",
            "Early stopping at epoch 136.\n",
            "epoch: 20, train_loss: 0.6238, train_accuracy: 67.85, val_loss: 0.7297, val_accuracy: 35.48\n",
            "epoch: 40, train_loss: 0.6082, train_accuracy: 68.11, val_loss: 0.7479, val_accuracy: 37.79\n",
            "epoch: 60, train_loss: 0.6029, train_accuracy: 67.85, val_loss: 0.7367, val_accuracy: 41.47\n",
            "epoch: 80, train_loss: 0.5998, train_accuracy: 67.98, val_loss: 0.7501, val_accuracy: 41.94\n",
            "epoch: 100, train_loss: 0.5974, train_accuracy: 68.24, val_loss: 0.7496, val_accuracy: 41.47\n",
            "Early stopping at epoch 116.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 545 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 545 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-206/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-207\n",
            "Parameters:  (0.001, 3, 16)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 421 operations to synchronize with Neptune. Do not kill this process.\n",
            "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread NeptuneAsyncOpProcessor:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 230, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 53, in run\n",
            "    self.work()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 246, in work\n",
            "    self.process_batch([element.obj for element in batch], batch[-1].ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 76, in wrapper\n",
            "    result = func(self_, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 259, in process_batch\n",
            "    processed_count, errors = self._processor._backend.execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 484, in execute_operations\n",
            "    self._execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/common/backends/utils.py\", line 65, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 642, in _execute_operations\n",
            "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 96, in __call__\n",
            "    future = self._api_method(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 271, in __call__\n",
            "    request_params = construct_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 312, in construct_request\n",
            "    construct_params(operation, request, op_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 335, in construct_params\n",
            "    marshal_param(param, param_value, request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado_core/param.py\", line 155, in marshal_param\n",
            "    request['data'] = json.dumps(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/__init__.py\", line 378, in dumps\n",
            "    return _default_encoder.encode(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 298, in encode\n",
            "    chunks = self.iterencode(o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 379, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "ValueError: Out of range float values are not JSON compliant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[95m\n",
            "----NeptuneSynchronizationAlreadyStopped---------------------------------------------------\n",
            "\u001b[0m\n",
            "The synchronization thread had stopped before Neptune could finish uploading the logged metadata.\n",
            "Your data is stored locally, but you'll need to finish the synchronization manually.\n",
            "To synchronize with the Neptune servers, enter the following on your command line:\n",
            "\n",
            "    \u001b[95mneptune sync\u001b[0m\n",
            "\n",
            "For details, see https://docs.neptune.ai/api/neptune_sync/\n",
            "\n",
            "If the synchronization fails, you may want to check your connection and ensure that you're\n",
            "within limits by going to your Neptune project settings -> Usage.\n",
            "If the issue persists, our support is happy to help.\n",
            "\n",
            "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting_help\n",
            "\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-207/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-208\n",
            "Parameters:  (0.001, 3, 32)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 54.7534, train_accuracy: 32.15, val_loss: 42.4941, val_accuracy: 64.52\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 117.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 106.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 451 operations to synchronize with Neptune. Do not kill this process.\n",
            "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread NeptuneAsyncOpProcessor:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 230, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 53, in run\n",
            "    self.work()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 246, in work\n",
            "    self.process_batch([element.obj for element in batch], batch[-1].ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 76, in wrapper\n",
            "    result = func(self_, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 259, in process_batch\n",
            "    processed_count, errors = self._processor._backend.execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 484, in execute_operations\n",
            "    self._execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/common/backends/utils.py\", line 65, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 642, in _execute_operations\n",
            "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 96, in __call__\n",
            "    future = self._api_method(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 271, in __call__\n",
            "    request_params = construct_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 312, in construct_request\n",
            "    construct_params(operation, request, op_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 335, in construct_params\n",
            "    marshal_param(param, param_value, request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado_core/param.py\", line 155, in marshal_param\n",
            "    request['data'] = json.dumps(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/__init__.py\", line 378, in dumps\n",
            "    return _default_encoder.encode(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 298, in encode\n",
            "    chunks = self.iterencode(o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 379, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "ValueError: Out of range float values are not JSON compliant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[95m\n",
            "----NeptuneSynchronizationAlreadyStopped---------------------------------------------------\n",
            "\u001b[0m\n",
            "The synchronization thread had stopped before Neptune could finish uploading the logged metadata.\n",
            "Your data is stored locally, but you'll need to finish the synchronization manually.\n",
            "To synchronize with the Neptune servers, enter the following on your command line:\n",
            "\n",
            "    \u001b[95mneptune sync\u001b[0m\n",
            "\n",
            "For details, see https://docs.neptune.ai/api/neptune_sync/\n",
            "\n",
            "If the synchronization fails, you may want to check your connection and ensure that you're\n",
            "within limits by going to your Neptune project settings -> Usage.\n",
            "If the issue persists, our support is happy to help.\n",
            "\n",
            "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting_help\n",
            "\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-208/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-209\n",
            "Parameters:  (0.001, 3, 64)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 426 operations to synchronize with Neptune. Do not kill this process.\n",
            "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread NeptuneAsyncOpProcessor:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 230, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 53, in run\n",
            "    self.work()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 246, in work\n",
            "    self.process_batch([element.obj for element in batch], batch[-1].ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 76, in wrapper\n",
            "    result = func(self_, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 259, in process_batch\n",
            "    processed_count, errors = self._processor._backend.execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 484, in execute_operations\n",
            "    self._execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/common/backends/utils.py\", line 65, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 642, in _execute_operations\n",
            "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 96, in __call__\n",
            "    future = self._api_method(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 271, in __call__\n",
            "    request_params = construct_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 312, in construct_request\n",
            "    construct_params(operation, request, op_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 335, in construct_params\n",
            "    marshal_param(param, param_value, request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado_core/param.py\", line 155, in marshal_param\n",
            "    request['data'] = json.dumps(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/__init__.py\", line 378, in dumps\n",
            "    return _default_encoder.encode(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 298, in encode\n",
            "    chunks = self.iterencode(o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 379, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "ValueError: Out of range float values are not JSON compliant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[95m\n",
            "----NeptuneSynchronizationAlreadyStopped---------------------------------------------------\n",
            "\u001b[0m\n",
            "The synchronization thread had stopped before Neptune could finish uploading the logged metadata.\n",
            "Your data is stored locally, but you'll need to finish the synchronization manually.\n",
            "To synchronize with the Neptune servers, enter the following on your command line:\n",
            "\n",
            "    \u001b[95mneptune sync\u001b[0m\n",
            "\n",
            "For details, see https://docs.neptune.ai/api/neptune_sync/\n",
            "\n",
            "If the synchronization fails, you may want to check your connection and ensure that you're\n",
            "within limits by going to your Neptune project settings -> Usage.\n",
            "If the issue persists, our support is happy to help.\n",
            "\n",
            "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting_help\n",
            "\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-209/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-210\n",
            "Parameters:  (0.001, 3, 128)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 103.\n",
            "epoch: 20, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: nan, train_accuracy: 67.85, val_loss: nan, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 413 operations to synchronize with Neptune. Do not kill this process.\n",
            "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread NeptuneAsyncOpProcessor:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 230, in run\n",
            "    super().run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 53, in run\n",
            "    self.work()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 246, in work\n",
            "    self.process_batch([element.obj for element in batch], batch[-1].ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/threading/daemon.py\", line 76, in wrapper\n",
            "    result = func(self_, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 259, in process_batch\n",
            "    processed_count, errors = self._processor._backend.execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 484, in execute_operations\n",
            "    self._execute_operations(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/common/backends/utils.py\", line 65, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 642, in _execute_operations\n",
            "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 96, in __call__\n",
            "    future = self._api_method(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 271, in __call__\n",
            "    request_params = construct_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 312, in construct_request\n",
            "    construct_params(operation, request, op_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado/client.py\", line 335, in construct_params\n",
            "    marshal_param(param, param_value, request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bravado_core/param.py\", line 155, in marshal_param\n",
            "    request['data'] = json.dumps(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/__init__.py\", line 378, in dumps\n",
            "    return _default_encoder.encode(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 298, in encode\n",
            "    chunks = self.iterencode(o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simplejson/encoder.py\", line 379, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "ValueError: Out of range float values are not JSON compliant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[95m\n",
            "----NeptuneSynchronizationAlreadyStopped---------------------------------------------------\n",
            "\u001b[0m\n",
            "The synchronization thread had stopped before Neptune could finish uploading the logged metadata.\n",
            "Your data is stored locally, but you'll need to finish the synchronization manually.\n",
            "To synchronize with the Neptune servers, enter the following on your command line:\n",
            "\n",
            "    \u001b[95mneptune sync\u001b[0m\n",
            "\n",
            "For details, see https://docs.neptune.ai/api/neptune_sync/\n",
            "\n",
            "If the synchronization fails, you may want to check your connection and ensure that you're\n",
            "within limits by going to your Neptune project settings -> Usage.\n",
            "If the issue persists, our support is happy to help.\n",
            "\n",
            "\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting_help\n",
            "\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-210/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-211\n",
            "Parameters:  (0.0001, 2, 16)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.6519, train_accuracy: 67.32, val_loss: 0.7344, val_accuracy: 63.13\n",
            "epoch: 40, train_loss: 0.6321, train_accuracy: 67.19, val_loss: 0.6535, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6293, train_accuracy: 67.06, val_loss: 0.6616, val_accuracy: 63.13\n",
            "epoch: 80, train_loss: 0.6273, train_accuracy: 67.19, val_loss: 0.6607, val_accuracy: 63.13\n",
            "epoch: 100, train_loss: 0.6259, train_accuracy: 67.19, val_loss: 0.6590, val_accuracy: 63.13\n",
            "Early stopping at epoch 116.\n",
            "epoch: 20, train_loss: 0.6395, train_accuracy: 67.59, val_loss: 0.6708, val_accuracy: 64.52\n",
            "epoch: 40, train_loss: 0.6240, train_accuracy: 67.45, val_loss: 0.6400, val_accuracy: 64.52\n",
            "epoch: 60, train_loss: 0.6223, train_accuracy: 67.45, val_loss: 0.6493, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6219, train_accuracy: 67.59, val_loss: 0.6478, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6217, train_accuracy: 67.59, val_loss: 0.6496, val_accuracy: 64.52\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: 0.6832, train_accuracy: 67.32, val_loss: 0.7255, val_accuracy: 63.59\n",
            "epoch: 40, train_loss: 0.6471, train_accuracy: 67.85, val_loss: 0.6819, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6301, train_accuracy: 67.85, val_loss: 0.6590, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6232, train_accuracy: 67.85, val_loss: 0.6489, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6203, train_accuracy: 67.98, val_loss: 0.6440, val_accuracy: 64.06\n",
            "Early stopping at epoch 110.\n",
            "epoch: 20, train_loss: 0.6535, train_accuracy: 67.59, val_loss: 0.6601, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6317, train_accuracy: 67.19, val_loss: 0.6423, val_accuracy: 65.44\n",
            "epoch: 60, train_loss: 0.6265, train_accuracy: 67.32, val_loss: 0.6436, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6243, train_accuracy: 67.98, val_loss: 0.6429, val_accuracy: 63.13\n",
            "epoch: 100, train_loss: 0.6233, train_accuracy: 67.32, val_loss: 0.6431, val_accuracy: 63.13\n",
            "Early stopping at epoch 113.\n",
            "epoch: 20, train_loss: 0.6331, train_accuracy: 67.85, val_loss: 0.6663, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6241, train_accuracy: 67.85, val_loss: 0.6511, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6195, train_accuracy: 67.85, val_loss: 0.6421, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6181, train_accuracy: 67.85, val_loss: 0.6387, val_accuracy: 64.52\n",
            "epoch: 100, train_loss: 0.6176, train_accuracy: 67.72, val_loss: 0.6371, val_accuracy: 64.98\n",
            "epoch: 120, train_loss: 0.6173, train_accuracy: 67.59, val_loss: 0.6363, val_accuracy: 64.98\n",
            "epoch: 140, train_loss: 0.6171, train_accuracy: 67.59, val_loss: 0.6358, val_accuracy: 64.52\n",
            "epoch: 160, train_loss: 0.6169, train_accuracy: 67.59, val_loss: 0.6355, val_accuracy: 64.06\n",
            "epoch: 180, train_loss: 0.6167, train_accuracy: 67.59, val_loss: 0.6353, val_accuracy: 64.06\n",
            "Early stopping at epoch 198.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 413 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 413 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-211/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-212\n",
            "Parameters:  (0.0001, 2, 32)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.6978, train_accuracy: 67.45, val_loss: 0.7398, val_accuracy: 63.13\n",
            "epoch: 40, train_loss: 0.6320, train_accuracy: 67.85, val_loss: 0.7493, val_accuracy: 64.52\n",
            "epoch: 60, train_loss: 0.6233, train_accuracy: 67.06, val_loss: 0.7139, val_accuracy: 64.98\n",
            "epoch: 80, train_loss: 0.6204, train_accuracy: 67.85, val_loss: 0.7218, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6196, train_accuracy: 67.72, val_loss: 0.7320, val_accuracy: 64.06\n",
            "Early stopping at epoch 110.\n",
            "epoch: 20, train_loss: 0.7024, train_accuracy: 66.40, val_loss: 0.7181, val_accuracy: 67.74\n",
            "epoch: 40, train_loss: 0.6629, train_accuracy: 67.45, val_loss: 0.6634, val_accuracy: 66.82\n",
            "epoch: 60, train_loss: 0.6301, train_accuracy: 67.32, val_loss: 0.6454, val_accuracy: 67.28\n",
            "epoch: 80, train_loss: 0.6216, train_accuracy: 66.40, val_loss: 0.6406, val_accuracy: 64.98\n",
            "epoch: 100, train_loss: 0.6197, train_accuracy: 67.06, val_loss: 0.6413, val_accuracy: 64.52\n",
            "epoch: 120, train_loss: 0.6190, train_accuracy: 67.98, val_loss: 0.6413, val_accuracy: 63.13\n",
            "Early stopping at epoch 127.\n",
            "epoch: 20, train_loss: 0.6316, train_accuracy: 67.85, val_loss: 0.6555, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6218, train_accuracy: 67.72, val_loss: 0.6573, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6202, train_accuracy: 67.59, val_loss: 0.6551, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6196, train_accuracy: 67.72, val_loss: 0.6520, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6192, train_accuracy: 67.85, val_loss: 0.6497, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: 0.6280, train_accuracy: 66.93, val_loss: 0.6511, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6244, train_accuracy: 66.80, val_loss: 0.6594, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6234, train_accuracy: 66.93, val_loss: 0.6531, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6227, train_accuracy: 67.06, val_loss: 0.6571, val_accuracy: 63.59\n",
            "epoch: 100, train_loss: 0.6220, train_accuracy: 67.06, val_loss: 0.6555, val_accuracy: 63.59\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: 0.7780, train_accuracy: 49.87, val_loss: 0.8164, val_accuracy: 51.61\n",
            "epoch: 40, train_loss: 0.6778, train_accuracy: 67.85, val_loss: 0.7244, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6389, train_accuracy: 67.85, val_loss: 0.6662, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6262, train_accuracy: 67.85, val_loss: 0.6498, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6228, train_accuracy: 67.59, val_loss: 0.6436, val_accuracy: 64.52\n",
            "epoch: 120, train_loss: 0.6217, train_accuracy: 67.59, val_loss: 0.6413, val_accuracy: 64.98\n",
            "epoch: 140, train_loss: 0.6211, train_accuracy: 67.72, val_loss: 0.6403, val_accuracy: 64.52\n",
            "epoch: 160, train_loss: 0.6206, train_accuracy: 67.45, val_loss: 0.6397, val_accuracy: 64.06\n",
            "epoch: 180, train_loss: 0.6202, train_accuracy: 67.45, val_loss: 0.6393, val_accuracy: 64.06\n",
            "epoch: 200, train_loss: 0.6198, train_accuracy: 67.45, val_loss: 0.6390, val_accuracy: 64.06\n",
            "Early stopping at epoch 212.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 455 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 455 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-212/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-213\n",
            "Parameters:  (0.0001, 2, 64)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.6663, train_accuracy: 67.45, val_loss: 0.6655, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6349, train_accuracy: 67.72, val_loss: 0.6447, val_accuracy: 66.36\n",
            "epoch: 60, train_loss: 0.6271, train_accuracy: 67.59, val_loss: 0.6424, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6244, train_accuracy: 67.06, val_loss: 0.6433, val_accuracy: 63.13\n",
            "epoch: 100, train_loss: 0.6229, train_accuracy: 66.93, val_loss: 0.6428, val_accuracy: 63.59\n",
            "epoch: 120, train_loss: 0.6218, train_accuracy: 66.93, val_loss: 0.6425, val_accuracy: 64.06\n",
            "Early stopping at epoch 125.\n",
            "epoch: 20, train_loss: 0.6252, train_accuracy: 67.72, val_loss: 0.6468, val_accuracy: 64.52\n",
            "epoch: 40, train_loss: 0.6238, train_accuracy: 67.72, val_loss: 0.6540, val_accuracy: 64.98\n",
            "epoch: 60, train_loss: 0.6225, train_accuracy: 68.11, val_loss: 0.6467, val_accuracy: 64.52\n",
            "epoch: 80, train_loss: 0.6217, train_accuracy: 67.85, val_loss: 0.6470, val_accuracy: 64.52\n",
            "epoch: 100, train_loss: 0.6210, train_accuracy: 67.85, val_loss: 0.6470, val_accuracy: 64.52\n",
            "Early stopping at epoch 111.\n",
            "epoch: 20, train_loss: 0.6585, train_accuracy: 66.93, val_loss: 0.6918, val_accuracy: 63.59\n",
            "epoch: 40, train_loss: 0.6277, train_accuracy: 66.93, val_loss: 0.6414, val_accuracy: 64.98\n",
            "epoch: 60, train_loss: 0.6217, train_accuracy: 67.06, val_loss: 0.6553, val_accuracy: 63.13\n",
            "epoch: 80, train_loss: 0.6201, train_accuracy: 67.06, val_loss: 0.6511, val_accuracy: 63.59\n",
            "epoch: 100, train_loss: 0.6193, train_accuracy: 67.59, val_loss: 0.6538, val_accuracy: 64.06\n",
            "epoch: 120, train_loss: 0.6187, train_accuracy: 67.85, val_loss: 0.6529, val_accuracy: 64.52\n",
            "Early stopping at epoch 127.\n",
            "epoch: 20, train_loss: 0.6475, train_accuracy: 67.85, val_loss: 0.6740, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6274, train_accuracy: 67.06, val_loss: 0.6428, val_accuracy: 63.59\n",
            "epoch: 60, train_loss: 0.6264, train_accuracy: 67.45, val_loss: 0.6408, val_accuracy: 63.59\n",
            "epoch: 80, train_loss: 0.6251, train_accuracy: 67.06, val_loss: 0.6405, val_accuracy: 62.67\n",
            "epoch: 100, train_loss: 0.6242, train_accuracy: 66.93, val_loss: 0.6399, val_accuracy: 63.13\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: 0.6266, train_accuracy: 67.32, val_loss: 0.6380, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6219, train_accuracy: 67.32, val_loss: 0.6383, val_accuracy: 62.67\n",
            "epoch: 60, train_loss: 0.6208, train_accuracy: 67.32, val_loss: 0.6368, val_accuracy: 62.67\n",
            "epoch: 80, train_loss: 0.6199, train_accuracy: 67.06, val_loss: 0.6368, val_accuracy: 63.13\n",
            "epoch: 100, train_loss: 0.6193, train_accuracy: 67.19, val_loss: 0.6362, val_accuracy: 63.13\n",
            "Early stopping at epoch 102.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 531 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 531 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-213/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-214\n",
            "Parameters:  (0.0001, 2, 128)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.6453, train_accuracy: 67.85, val_loss: 0.6629, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6213, train_accuracy: 67.32, val_loss: 0.6322, val_accuracy: 69.59\n",
            "epoch: 60, train_loss: 0.6154, train_accuracy: 67.85, val_loss: 0.6271, val_accuracy: 65.44\n",
            "epoch: 80, train_loss: 0.6140, train_accuracy: 68.11, val_loss: 0.6264, val_accuracy: 65.90\n",
            "epoch: 100, train_loss: 0.6128, train_accuracy: 68.11, val_loss: 0.6252, val_accuracy: 66.36\n",
            "epoch: 120, train_loss: 0.6117, train_accuracy: 68.11, val_loss: 0.6246, val_accuracy: 65.90\n",
            "Early stopping at epoch 139.\n",
            "epoch: 20, train_loss: 0.6300, train_accuracy: 66.80, val_loss: 0.6499, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6281, train_accuracy: 67.32, val_loss: 0.6849, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6228, train_accuracy: 67.45, val_loss: 0.6789, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6213, train_accuracy: 67.06, val_loss: 0.6697, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6201, train_accuracy: 67.06, val_loss: 0.6673, val_accuracy: 64.06\n",
            "Early stopping at epoch 109.\n",
            "epoch: 20, train_loss: 0.6342, train_accuracy: 67.85, val_loss: 0.6623, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6214, train_accuracy: 66.80, val_loss: 0.6417, val_accuracy: 66.36\n",
            "epoch: 60, train_loss: 0.6189, train_accuracy: 68.11, val_loss: 0.6402, val_accuracy: 65.44\n",
            "epoch: 80, train_loss: 0.6179, train_accuracy: 67.45, val_loss: 0.6393, val_accuracy: 64.98\n",
            "epoch: 100, train_loss: 0.6170, train_accuracy: 67.85, val_loss: 0.6380, val_accuracy: 65.44\n",
            "epoch: 120, train_loss: 0.6163, train_accuracy: 67.85, val_loss: 0.6370, val_accuracy: 64.98\n",
            "epoch: 140, train_loss: 0.6156, train_accuracy: 67.72, val_loss: 0.6362, val_accuracy: 65.44\n",
            "Early stopping at epoch 152.\n",
            "epoch: 20, train_loss: 0.6155, train_accuracy: 67.59, val_loss: 0.6298, val_accuracy: 65.44\n",
            "epoch: 40, train_loss: 0.6116, train_accuracy: 67.59, val_loss: 0.6278, val_accuracy: 65.44\n",
            "epoch: 60, train_loss: 0.6108, train_accuracy: 67.72, val_loss: 0.6282, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6101, train_accuracy: 67.85, val_loss: 0.6277, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6094, train_accuracy: 67.85, val_loss: 0.6270, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: 0.6364, train_accuracy: 67.85, val_loss: 0.6530, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6195, train_accuracy: 67.45, val_loss: 0.6478, val_accuracy: 67.74\n",
            "epoch: 60, train_loss: 0.6151, train_accuracy: 68.24, val_loss: 0.6441, val_accuracy: 68.20\n",
            "epoch: 80, train_loss: 0.6140, train_accuracy: 68.11, val_loss: 0.6436, val_accuracy: 69.12\n",
            "epoch: 100, train_loss: 0.6131, train_accuracy: 67.85, val_loss: 0.6436, val_accuracy: 67.74\n",
            "epoch: 120, train_loss: 0.6122, train_accuracy: 67.98, val_loss: 0.6431, val_accuracy: 68.66\n",
            "epoch: 140, train_loss: 0.6115, train_accuracy: 68.11, val_loss: 0.6427, val_accuracy: 68.66\n",
            "epoch: 160, train_loss: 0.6108, train_accuracy: 68.24, val_loss: 0.6424, val_accuracy: 68.20\n",
            "Early stopping at epoch 177.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 563 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 563 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-214/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-215\n",
            "Parameters:  (0.0001, 3, 16)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 2.7953, train_accuracy: 67.85, val_loss: 8.2257, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 1.4829, train_accuracy: 32.94, val_loss: 3.0541, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6606, train_accuracy: 67.85, val_loss: 1.3879, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6254, train_accuracy: 67.85, val_loss: 1.0326, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6229, train_accuracy: 67.85, val_loss: 1.0283, val_accuracy: 64.06\n",
            "Early stopping at epoch 105.\n",
            "epoch: 20, train_loss: 0.6811, train_accuracy: 66.14, val_loss: 2.1787, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6231, train_accuracy: 67.06, val_loss: 1.0226, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6202, train_accuracy: 67.85, val_loss: 0.8134, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6196, train_accuracy: 67.32, val_loss: 0.8600, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6194, train_accuracy: 67.32, val_loss: 0.8484, val_accuracy: 35.94\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: 1.5946, train_accuracy: 67.85, val_loss: 0.9800, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 3.4755, train_accuracy: 67.85, val_loss: 0.6410, val_accuracy: 64.52\n",
            "epoch: 60, train_loss: 0.6342, train_accuracy: 67.85, val_loss: 0.8808, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6219, train_accuracy: 67.59, val_loss: 0.6825, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6196, train_accuracy: 66.93, val_loss: 0.6546, val_accuracy: 64.06\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: 6.6176, train_accuracy: 67.85, val_loss: 2.6874, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 2.4262, train_accuracy: 67.85, val_loss: 2.9890, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 1.0009, train_accuracy: 67.85, val_loss: 1.8947, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6765, train_accuracy: 67.85, val_loss: 0.7811, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6340, train_accuracy: 67.85, val_loss: 0.6905, val_accuracy: 54.84\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: 0.6682, train_accuracy: 67.85, val_loss: 1.1853, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6296, train_accuracy: 67.85, val_loss: 0.6960, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6247, train_accuracy: 66.40, val_loss: 0.6578, val_accuracy: 66.82\n",
            "epoch: 80, train_loss: 0.6239, train_accuracy: 66.40, val_loss: 0.6430, val_accuracy: 64.52\n",
            "epoch: 100, train_loss: 0.6237, train_accuracy: 66.80, val_loss: 0.6395, val_accuracy: 63.13\n",
            "epoch: 120, train_loss: 0.6235, train_accuracy: 66.80, val_loss: 0.6393, val_accuracy: 63.13\n",
            "epoch: 140, train_loss: 0.6233, train_accuracy: 66.80, val_loss: 0.6392, val_accuracy: 63.13\n",
            "Early stopping at epoch 158.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 427 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 427 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-215/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-216\n",
            "Parameters:  (0.0001, 3, 32)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.6673, train_accuracy: 67.85, val_loss: 0.6681, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6242, train_accuracy: 67.85, val_loss: 0.6550, val_accuracy: 67.28\n",
            "epoch: 60, train_loss: 0.6196, train_accuracy: 67.85, val_loss: 0.7717, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6195, train_accuracy: 67.85, val_loss: 0.7954, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6193, train_accuracy: 67.85, val_loss: 0.7810, val_accuracy: 35.94\n",
            "epoch: 120, train_loss: 0.6192, train_accuracy: 67.85, val_loss: 0.7737, val_accuracy: 35.94\n",
            "epoch: 140, train_loss: 0.6192, train_accuracy: 67.85, val_loss: 0.7729, val_accuracy: 35.94\n",
            "Early stopping at epoch 142.\n",
            "epoch: 20, train_loss: 0.6195, train_accuracy: 67.19, val_loss: 2.3021, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6210, train_accuracy: 66.93, val_loss: 2.3302, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6185, train_accuracy: 67.98, val_loss: 2.0691, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6178, train_accuracy: 67.85, val_loss: 2.1544, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6175, train_accuracy: 67.72, val_loss: 2.1609, val_accuracy: 35.94\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: 0.7608, train_accuracy: 67.85, val_loss: 0.7472, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.7468, train_accuracy: 67.85, val_loss: 1.9216, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6282, train_accuracy: 67.85, val_loss: 1.0076, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6209, train_accuracy: 67.45, val_loss: 0.7836, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6206, train_accuracy: 67.45, val_loss: 0.7939, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "epoch: 20, train_loss: 0.6276, train_accuracy: 67.85, val_loss: 1.6947, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6201, train_accuracy: 67.59, val_loss: 2.4317, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6189, train_accuracy: 68.24, val_loss: 2.2752, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6185, train_accuracy: 67.72, val_loss: 2.1693, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6183, train_accuracy: 68.11, val_loss: 2.1726, val_accuracy: 35.94\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: 0.8885, train_accuracy: 41.08, val_loss: 2.1545, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6731, train_accuracy: 67.32, val_loss: 0.9933, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6221, train_accuracy: 67.72, val_loss: 0.6445, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6206, train_accuracy: 67.85, val_loss: 0.6530, val_accuracy: 65.90\n",
            "epoch: 100, train_loss: 0.6203, train_accuracy: 67.85, val_loss: 0.6493, val_accuracy: 64.98\n",
            "epoch: 120, train_loss: 0.6201, train_accuracy: 67.72, val_loss: 0.6459, val_accuracy: 64.52\n",
            "epoch: 140, train_loss: 0.6200, train_accuracy: 67.45, val_loss: 0.6443, val_accuracy: 64.52\n",
            "epoch: 160, train_loss: 0.6198, train_accuracy: 67.45, val_loss: 0.6434, val_accuracy: 64.06\n",
            "Early stopping at epoch 178.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 577 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 577 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-216/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-217\n",
            "Parameters:  (0.0001, 3, 64)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.6298, train_accuracy: 67.85, val_loss: 0.7300, val_accuracy: 32.26\n",
            "epoch: 40, train_loss: 0.6244, train_accuracy: 66.80, val_loss: 0.8700, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6237, train_accuracy: 66.93, val_loss: 0.8586, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6230, train_accuracy: 67.32, val_loss: 0.8473, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6225, train_accuracy: 67.19, val_loss: 0.8527, val_accuracy: 35.94\n",
            "Early stopping at epoch 111.\n",
            "epoch: 20, train_loss: 0.6253, train_accuracy: 67.85, val_loss: 0.8767, val_accuracy: 35.48\n",
            "epoch: 40, train_loss: 0.6200, train_accuracy: 67.19, val_loss: 1.1587, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6188, train_accuracy: 67.45, val_loss: 1.0649, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6184, train_accuracy: 67.72, val_loss: 1.0344, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6181, train_accuracy: 67.72, val_loss: 1.0448, val_accuracy: 35.94\n",
            "Early stopping at epoch 104.\n",
            "epoch: 20, train_loss: 0.6402, train_accuracy: 67.85, val_loss: 0.7785, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6205, train_accuracy: 67.85, val_loss: 0.6635, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6202, train_accuracy: 67.72, val_loss: 0.6518, val_accuracy: 63.59\n",
            "epoch: 80, train_loss: 0.6198, train_accuracy: 67.59, val_loss: 0.6552, val_accuracy: 63.59\n",
            "epoch: 100, train_loss: 0.6195, train_accuracy: 67.72, val_loss: 0.6576, val_accuracy: 63.59\n",
            "epoch: 120, train_loss: 0.6193, train_accuracy: 67.72, val_loss: 0.6588, val_accuracy: 63.59\n",
            "Early stopping at epoch 139.\n",
            "epoch: 20, train_loss: 0.6352, train_accuracy: 67.85, val_loss: 0.7862, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6285, train_accuracy: 67.06, val_loss: 0.6464, val_accuracy: 63.59\n",
            "epoch: 60, train_loss: 0.6243, train_accuracy: 67.06, val_loss: 0.6497, val_accuracy: 63.59\n",
            "epoch: 80, train_loss: 0.6234, train_accuracy: 67.06, val_loss: 0.6548, val_accuracy: 64.52\n",
            "epoch: 100, train_loss: 0.6227, train_accuracy: 67.06, val_loss: 0.6525, val_accuracy: 64.52\n",
            "epoch: 120, train_loss: 0.6221, train_accuracy: 67.06, val_loss: 0.6515, val_accuracy: 64.52\n",
            "Early stopping at epoch 129.\n",
            "epoch: 20, train_loss: 0.6592, train_accuracy: 67.59, val_loss: 0.8547, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6298, train_accuracy: 67.06, val_loss: 0.7319, val_accuracy: 64.52\n",
            "epoch: 60, train_loss: 0.6258, train_accuracy: 66.93, val_loss: 0.7363, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6246, train_accuracy: 67.06, val_loss: 0.7467, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6236, train_accuracy: 67.06, val_loss: 0.7591, val_accuracy: 64.06\n",
            "Early stopping at epoch 102.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 416 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 416 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-217/metadata\n",
            "finished, submitting to neptune...\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-218\n",
            "Parameters:  (0.0001, 3, 128)\n",
            "71\n",
            "51\n",
            "epoch: 20, train_loss: 0.7367, train_accuracy: 45.54, val_loss: 1.6796, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6297, train_accuracy: 67.32, val_loss: 1.2643, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6200, train_accuracy: 67.59, val_loss: 1.1695, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6191, train_accuracy: 67.32, val_loss: 1.2197, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6183, train_accuracy: 67.45, val_loss: 1.1918, val_accuracy: 35.94\n",
            "Early stopping at epoch 101.\n",
            "epoch: 20, train_loss: 2.5164, train_accuracy: 32.15, val_loss: 1.2878, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 2.5012, train_accuracy: 32.15, val_loss: 0.8433, val_accuracy: 64.06\n",
            "epoch: 60, train_loss: 0.6614, train_accuracy: 66.40, val_loss: 0.7052, val_accuracy: 50.23\n",
            "epoch: 80, train_loss: 0.6233, train_accuracy: 67.45, val_loss: 0.7532, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6224, train_accuracy: 67.72, val_loss: 0.7725, val_accuracy: 64.06\n",
            "epoch: 120, train_loss: 0.6214, train_accuracy: 67.32, val_loss: 0.7317, val_accuracy: 64.06\n",
            "epoch: 140, train_loss: 0.6208, train_accuracy: 67.59, val_loss: 0.7351, val_accuracy: 64.06\n",
            "Early stopping at epoch 148.\n",
            "epoch: 20, train_loss: 0.6225, train_accuracy: 67.59, val_loss: 0.7148, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6216, train_accuracy: 67.85, val_loss: 0.7733, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6204, train_accuracy: 67.72, val_loss: 0.7369, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6199, train_accuracy: 67.72, val_loss: 0.7384, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6194, train_accuracy: 67.59, val_loss: 0.7440, val_accuracy: 35.94\n",
            "Early stopping at epoch 115.\n",
            "epoch: 20, train_loss: 0.6231, train_accuracy: 67.06, val_loss: 1.0620, val_accuracy: 35.94\n",
            "epoch: 40, train_loss: 0.6223, train_accuracy: 67.32, val_loss: 1.0038, val_accuracy: 35.94\n",
            "epoch: 60, train_loss: 0.6215, train_accuracy: 67.32, val_loss: 0.9457, val_accuracy: 35.94\n",
            "epoch: 80, train_loss: 0.6208, train_accuracy: 66.93, val_loss: 0.9752, val_accuracy: 35.94\n",
            "epoch: 100, train_loss: 0.6202, train_accuracy: 67.45, val_loss: 0.9677, val_accuracy: 35.94\n",
            "Early stopping at epoch 105.\n",
            "epoch: 20, train_loss: 0.6236, train_accuracy: 67.85, val_loss: 0.7293, val_accuracy: 64.06\n",
            "epoch: 40, train_loss: 0.6210, train_accuracy: 68.11, val_loss: 0.6936, val_accuracy: 64.98\n",
            "epoch: 60, train_loss: 0.6192, train_accuracy: 67.72, val_loss: 0.7097, val_accuracy: 64.06\n",
            "epoch: 80, train_loss: 0.6185, train_accuracy: 67.98, val_loss: 0.7146, val_accuracy: 64.06\n",
            "epoch: 100, train_loss: 0.6180, train_accuracy: 67.72, val_loss: 0.7117, val_accuracy: 64.06\n",
            "epoch: 120, train_loss: 0.6176, train_accuracy: 67.85, val_loss: 0.7098, val_accuracy: 64.06\n",
            "Early stopping at epoch 139.\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 595 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 595 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/miyamura80/Slither-Graph-CFG/e/SLIT-218/metadata\n",
            "finished, submitting to neptune...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export NEPTUNE_API_TOKEN=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyOTFjMWUxZi1iMWUyLTQ5YTctYmU3Yy05MWI4ZThlMjcyMDgifQ==\""
      ],
      "metadata": {
        "id": "rJNeciV205_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!neptune sync"
      ],
      "metadata": {
        "id": "uFUudAX7rA89"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}