{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Se0pqteEDOQ",
        "outputId": "b4da5ee7-2c5e-4214-f2f9-5afac2a00765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.1/448.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q dm-haiku spektral optax neptune flax torch torch-geometric jraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U jax jaxlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlEyTu4ZEP7a",
        "outputId": "0a97da0a-0cf7-47c4-9581-b223c0989a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BzBO2qxER4l",
        "outputId": "be6d3f21-c087-4eef-c012-302c7b0aad9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda11_cudnn82] in /usr/local/lib/python3.10/dist-packages (0.4.10)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (0.1.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax[cuda11_cudnn82]) (1.10.1)\n",
            "INFO: pip is looking at multiple versions of jax[cuda11-cudnn82] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax[cuda11_cudnn82]\n",
            "  Using cached jax-0.4.10-py3-none-any.whl\n",
            "  Downloading jax-0.4.9.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxlib==0.4.7+cuda11.cudnn82 (from jax[cuda11_cudnn82])\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.7%2Bcuda11.cudnn82-cp310-cp310-manylinux2014_x86_64.whl (149.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=65560fab0bf57579bc302d7d3a5643bc21cd2218f7b5457f705126ad6d3c52e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/6f/35/a8fac8b61de8e0d9eb07988481528898561923e260b1fa7d2f\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.10\n",
            "    Uninstalling jaxlib-0.4.10:\n",
            "      Successfully uninstalled jaxlib-0.4.10\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.10\n",
            "    Uninstalling jax-0.4.10:\n",
            "      Successfully uninstalled jax-0.4.10\n",
            "Successfully installed jax-0.4.8 jaxlib-0.4.7+cuda11.cudnn82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import neptune\n",
        "import statistics\n",
        "import itertools\n",
        "import pickle\n",
        "import networkx as nx\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "import jraph\n",
        "import haiku as hk\n",
        "import functools\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from typing import Sequence\n"
      ],
      "metadata": {
        "id": "axy9gTTqETmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "jZhDpUPSEZMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "xpyPrTOQYg-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.token2idx = defaultdict(lambda: len(self.token2idx))\n",
        "        self.idx2token = {}\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        for text in texts:\n",
        "            self.tokenize(text)\n",
        "        self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
        "\n",
        "    def tokenize(self, texts):\n",
        "        all_tokens = set()\n",
        "        for text in texts:\n",
        "            tokens = text.split()\n",
        "            all_tokens.update(tokens)\n",
        "\n",
        "        # Create token to index mapping\n",
        "        self.token2idx = {token: idx for idx, token in enumerate(sorted(all_tokens))}\n",
        "        self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
        "\n",
        "        # Tokenize texts\n",
        "        tokenized_texts = [[self.token2idx[token] for token in text.split()] for text in texts]\n",
        "\n",
        "        return tokenized_texts\n",
        "\n",
        "    def detokenize(self, sequences):\n",
        "        # Detokenize sequences\n",
        "        detokenized_texts = [\" \".join(self.idx2token[token] for token in sequence) for sequence in sequences]\n",
        "\n",
        "        return detokenized_texts\n",
        "\n",
        "# Initialize the tokenizer and fit on the texts\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "texts = [\n",
        "    \"PUSH1 0x0 CALLDATALOAD PUSH29 0x100000000000000000000000000000000000000000000000000000000 SWAP1 DIV PUSH4 0xffffffff AND DUP1 PUSH4 0xda51bbd9 EQ PUSH1 0x44 JUMPI\",\n",
        "    \"JUMPDEST PUSH1 0x0 SLOAD DUP2 JUMP\",\n",
        "    \"PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x4 CALLDATASIZE LT PUSH1 0x3f JUMPI\",\n",
        "]\n",
        "\n",
        "tokenized_texts = tokenizer.tokenize(texts)\n",
        "\n",
        "# # Tokenize the texts\n",
        "# tokenized_texts = [tokenizer.tokenize(text) for text in texts]\n",
        "\n",
        "# # Detokenize the texts\n",
        "# detokenized_texts = [tokenizer.detokenize(tokens) for tokens in tokenized_texts]\n",
        "\n",
        "# print(tokenized_texts)\n",
        "# print(detokenized_texts)\n"
      ],
      "metadata": {
        "id": "TRu3AOAVYiOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder\n"
      ],
      "metadata": {
        "id": "syUDLnBGEbN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(hk.Module):\n",
        "  def __init__(self, hidden_dim, num_heads, dropout_rate, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def __call__(self, x, mask):\n",
        "    attention = hk.MultiHeadAttention(num_heads=self.num_heads, key_size=self.hidden_dim, w_init=hk.initializers.TruncatedNormal(1.0), name='attention')\n",
        "    feed_forward = hk.nets.MLP([self.hidden_dim, self.hidden_dim], name='feed_forward')\n",
        "\n",
        "    mask = mask[:, None, None, :]  # reshape the mask to [batch_size, 1, 1, sequence_length]\n",
        "    attn_output = attention(query=x, key=x, value=x, mask=mask)\n",
        "    attn_output = hk.Linear(self.hidden_dim, name=\"linear\")(attn_output)  # Apply the linear transformation\n",
        "    attn_output = hk.Linear(x.shape[-1], name=\"linear_after_attn\")(attn_output)  # New Linear layer to match the dimensions of x\n",
        "    x = x + hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_output)\n",
        "    ffn_output = feed_forward(x)\n",
        "    ffn_output = hk.Linear(x.shape[-1], name=\"linear_after_ffn\")(ffn_output)  # Linear layer to match the dimensions of x\n",
        "    ffn_output = hk.dropout(hk.next_rng_key(), self.dropout_rate, ffn_output)\n",
        "    x = x + ffn_output\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class TransformerAutoEncoder(hk.Module):\n",
        "  def __init__(self, hidden_dim, num_layers, num_heads, dropout_rate, bottleneck_dim, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.bottleneck_dim = bottleneck_dim\n",
        "\n",
        "  def positional_encoding(self, length, d_model):\n",
        "    angle_rads = self.get_angles(np.arange(length)[:, np.newaxis],\n",
        "                                  np.arange(d_model)[np.newaxis, :],\n",
        "                                  d_model)\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return jnp.array(pos_encoding, dtype=float)\n",
        "\n",
        "  def get_angles(self, pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "  def __call__(self, x, mask):\n",
        "    positional_encoding = self.positional_encoding(x.shape[1], x.shape[2])\n",
        "\n",
        "    x = x + positional_encoding\n",
        "    x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
        "\n",
        "    # Encoder\n",
        "    hidden_dim = self.hidden_dim\n",
        "    for i in range(self.num_layers//2):  # Half of the layers are used for encoding\n",
        "      x = TransformerBlock(hidden_dim, self.num_heads, self.dropout_rate, name=f'encoder_layer_{i}')(x, mask)\n",
        "      hidden_dim = hidden_dim // 2  # Reduce the hidden dimension by a factor of 1/2 for each layer\n",
        "\n",
        "    # Bottleneck\n",
        "    x = hk.Linear(self.bottleneck_dim, name='bottleneck')(x)\n",
        "\n",
        "    # Transform bottleneck output back to hidden_dim before feeding to the decoder\n",
        "    x = hk.Linear(hidden_dim, name='expand')(x)\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(self.num_layers//2, self.num_layers):  # Half of the layers are used for decoding\n",
        "      hidden_dim = hidden_dim * 2  # Increase the hidden dimension by a factor of 2 for each layer\n",
        "      x = TransformerBlock(hidden_dim, self.num_heads, self.dropout_rate, name=f'decoder_layer_{i}')(x, mask)\n",
        "\n",
        "    # Output layer to reconstruct the original input size\n",
        "    output = hk.Linear(self.hidden_dim, name='output')(x)  # Here, self.hidden_dim represents the original input size\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ev5rTwbv6T3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "def model_fn(rng, x, mask):\n",
        "  return TransformerAutoEncoder(hidden_dim=256, num_layers=2, num_heads=8, dropout_rate=0.1, bottleneck_dim=128)(x, mask)\n",
        "\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "model_fn_partial = functools.partial(model_fn, rng)\n",
        "\n",
        "# Transform the model function into a pair of pure functions\n",
        "model = hk.transform(model_fn_partial)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(params, x, mask):\n",
        "  x_recon = model.apply(params, x, mask)\n",
        "  loss = jnp.mean((x_recon - x) ** 2)\n",
        "  return loss\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = optax.adam(0.001)\n",
        "\n",
        "# Initialize the parameters\n",
        "\n",
        "params = model.init(rng, jnp.ones((1, 10, 256)), jnp.zeros((1, 10), dtype=bool))  # Assuming mask is a boolean tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "CZ1VCDmA7LMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax.nn import one_hot\n",
        "\n",
        "def encoder_forward(hidden_dim, vocab_size, x):\n",
        "    x_one_hot = one_hot(x, vocab_size)\n",
        "    x_flat = x_one_hot.reshape((x.shape[0], -1))\n",
        "    return hk.Linear(hidden_dim)(x_flat)\n",
        "\n",
        "def decoder_forward(hidden_dim, vocab_size, max_seq_length, z):\n",
        "    decoded = hk.Linear(max_seq_length * vocab_size)(z)\n",
        "    return decoded.reshape((-1, max_seq_length, vocab_size))\n",
        "\n",
        "class Encoder(hk.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size, max_seq_length, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return encoder_forward(self.hidden_dim, self.vocab_size, x)\n",
        "\n",
        "class Decoder(hk.Module):\n",
        "    def __init__(self, hidden_dim, vocab_size, max_seq_length, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __call__(self, z):\n",
        "        return decoder_forward(self.hidden_dim, self.vocab_size, self.max_seq_length, z)\n"
      ],
      "metadata": {
        "id": "08rbeb2DEX9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def autoencoder_fn(x, hidden_dim, vocab_size, max_seq_length):\n",
        "    # Encoder\n",
        "    x_one_hot = one_hot(x, vocab_size)\n",
        "    x_flat = x_one_hot.reshape((x.shape[0], -1))\n",
        "    z = hk.Linear(hidden_dim)(x_flat)\n",
        "    # Decoder\n",
        "    decoded = hk.Linear(max_seq_length * vocab_size)(z)\n",
        "    x_reconstructed = decoded.reshape((-1, max_seq_length, vocab_size))\n",
        "    return x_reconstructed\n",
        "\n",
        "def cross_entropy(log_probs, targets):\n",
        "    # Get the log-probabilities of the targets\n",
        "    log_probs_target = jnp.take_along_axis(log_probs, targets[..., None], axis=-1)\n",
        "    # Compute the mean negative log-probability\n",
        "    return -jnp.mean(log_probs_target)\n",
        "\n",
        "def reconstruction_loss(x, params, rng):\n",
        "    logits = apply_fn(params, rng, x)\n",
        "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "    # Compute the loss for each example in the batch\n",
        "    losses = vmap(cross_entropy)(log_probs, x)\n",
        "    # Compute the mean loss over the batch\n",
        "    return jnp.mean(losses)"
      ],
      "metadata": {
        "id": "m4VQLZcoeQyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad, jit, vmap\n",
        "from functools import partial\n",
        "\n",
        "# Get the maximum sequence length from the tokenized data\n",
        "max_seq_length = max(len(seq) for seq in tokenized_texts)\n",
        "\n",
        "# Pad the sequences to max_seq_length\n",
        "tokenized_texts_padded = [seq + [0] * (max_seq_length - len(seq)) for seq in tokenized_texts]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "tokenized_texts_padded = jnp.array(tokenized_texts_padded)\n",
        "print(tokenized_texts_padded)\n",
        "\n",
        "# Transform the model into a function that can be called with parameters and inputs\n",
        "# This gives us a pair of pure functions (init and apply)\n",
        "hidden_dim = 128\n",
        "vocab_size = 10000\n",
        "max_seq_length = 50\n",
        "autoencoder_fn_partial = partial(autoencoder_fn, hidden_dim=hidden_dim, vocab_size=vocab_size, max_seq_length=max_seq_length)\n",
        "init_fn, apply_fn = hk.transform(autoencoder_fn_partial)\n",
        "\n",
        "# Initialize parameters\n",
        "rng = jax.random.PRNGKey(0)  # Replace with your own key, if necessary\n",
        "x = jnp.ones((1, max_seq_length), dtype=jnp.int32)  # Replace with your actual data\n",
        "params = init_fn(rng, x)\n",
        "\n",
        "# Run inference\n",
        "rng_inference = jax.random.PRNGKey(1)  # Different RNG for inference\n",
        "x_reconstructed = apply_fn(params, rng_inference, x)\n",
        "\n",
        "# Compute loss\n",
        "loss = reconstruction_loss(x, params, rng_inference)\n",
        "print('Reconstruction loss:', loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mMFbcMmedYO",
        "outputId": "60ffefc2-f84c-4b88-8cff-a3fbd4e59bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21  0 10 22  1 25 12 23  8  9 13 23  7 15 21  5 18]\n",
            " [17 21  0 24 14 16  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [21  6 21  4 20 21  3 11 19 21  2 18  0  0  0  0  0]]\n",
            "Reconstruction loss: 9.208459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from flax import linen as nn\n",
        "from jax.nn import one_hot\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    hidden_dim: int\n",
        "    vocab_size: int\n",
        "    max_seq_length: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense = nn.Dense(self.hidden_dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x_one_hot = one_hot(x, self.vocab_size)\n",
        "        x_flat = x_one_hot.reshape((x.shape[0], -1))\n",
        "        return self.dense(x_flat)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    hidden_dim: int\n",
        "    vocab_size: int\n",
        "    max_seq_length: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense = nn.Dense(self.max_seq_length * self.vocab_size)\n",
        "\n",
        "    def __call__(self, z):\n",
        "        decoded = self.dense(z)\n",
        "        return decoded.reshape((-1, self.max_seq_length, self.vocab_size))"
      ],
      "metadata": {
        "id": "f4FORL70xpnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the maximum sequence length from the tokenized data\n",
        "max_seq_length = max(len(seq) for seq in tokenized_texts)\n",
        "\n",
        "# Pad the sequences to max_seq_length\n",
        "tokenized_texts_padded = [seq + [0] * (max_seq_length - len(seq)) for seq in tokenized_texts]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "tokenized_texts_padded = jnp.array(tokenized_texts_padded)\n",
        "print(tokenized_texts_padded)\n",
        "\n",
        "\n",
        "# Initialize the models\n",
        "encoder = Encoder(hidden_dim=128, vocab_size=len(tokenizer.token2idx), max_seq_length=max_seq_length)\n",
        "decoder = Decoder(hidden_dim=128, vocab_size=len(tokenizer.token2idx), max_seq_length=max_seq_length)\n",
        "\n",
        "# Initialize parameters\n",
        "encoder_params = encoder.init(jax.random.PRNGKey(0), tokenized_texts_padded)\n",
        "decoder_params = decoder.init(jax.random.PRNGKey(1), jnp.ones((len(tokenized_texts_padded), 128)))  # assuming the output of the encoder is of size (batch_size, 128)\n",
        "\n",
        "# Compile the models\n",
        "encode = jax.jit(encoder.apply)\n",
        "decode = jax.jit(decoder.apply)\n",
        "\n",
        "# Encode and decode\n",
        "encoded = encode(encoder_params, tokenized_texts_padded)\n",
        "decoded = decode(decoder_params, encoded)\n",
        "\n",
        "# Get the most probable token IDs\n",
        "decoded_ids = jnp.argmax(decoded, axis=-1)\n",
        "\n",
        "x = [seq.tolist() for seq in decoded_ids]\n",
        "print(x)\n",
        "\n",
        "# Detokenize\n",
        "detokenized_texts = tokenizer.detokenize(x)\n",
        "texts = [\n",
        "    \"PUSH1 0x0 CALLDATALOAD PUSH29 0x100000000000000000000000000000000000000000000000000000000 SWAP1 DIV PUSH4 0xffffffff AND DUP1 PUSH4 0xda51bbd9 EQ PUSH1 0x44 JUMPI\",\n",
        "    \"JUMPDEST PUSH1 0x0 SLOAD DUP2 JUMP\",\n",
        "    \"PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x4 CALLDATASIZE LT PUSH1 0x3f JUMPI\",\n",
        "]\n",
        "\n",
        "print(\"\\n\".join(detokenized_texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4MOLdYOwg9u",
        "outputId": "ce97d146-194a-42c8-c62f-271f74665a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21  0 10 22  1 25 12 23  8  9 13 23  7 15 21  5 18]\n",
            " [17 21  0 24 14 16  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [21  6 21  4 20 21  3 11 19 21  2 18  0  0  0  0  0]]\n",
            "[[13, 4, 13, 25, 5, 20, 17, 10, 1, 14, 3, 19, 1, 13, 13, 15, 12], [14, 4, 0, 23, 15, 2, 11, 5, 21, 22, 6, 10, 11, 21, 16, 7, 19], [5, 3, 20, 20, 25, 19, 14, 12, 11, 22, 4, 18, 17, 21, 1, 7, 11]]\n",
            "DUP1 0x40 DUP1 SWAP1 0x44 MSTORE JUMPDEST CALLDATALOAD 0x100000000000000000000000000000000000000000000000000000000 DUP2 0x4 LT 0x100000000000000000000000000000000000000000000000000000000 DUP1 DUP1 EQ DIV\n",
            "DUP2 0x40 0x0 PUSH4 EQ 0x3f CALLDATASIZE 0x44 PUSH1 PUSH29 0x80 CALLDATALOAD CALLDATASIZE PUSH1 JUMP 0xda51bbd9 LT\n",
            "0x44 0x4 MSTORE MSTORE SWAP1 LT DUP2 DIV CALLDATASIZE PUSH29 0x40 JUMPI JUMPDEST PUSH1 0x100000000000000000000000000000000000000000000000000000000 0xda51bbd9 CALLDATASIZE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits, targets):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss.\n",
        "    Args:\n",
        "        logits: [batch, length, num_classes] float array.\n",
        "        targets: [batch, length] integer array.\n",
        "    Returns:\n",
        "        scalar float loss.\n",
        "    \"\"\"\n",
        "    one_hot_targets = jax.nn.one_hot(targets, logits.shape[-1])\n",
        "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "    return -jnp.sum(one_hot_targets * log_probs) / targets.size\n",
        "\n",
        "# Compute loss\n",
        "loss = cross_entropy_loss(decoded, tokenized_texts_padded)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G0rRIU0b2d6",
        "outputId": "fbe74342-fad7-4850-9090-99a2f3d52748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2794256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8rjfziOd06u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "gRyLk0ABb_Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vOEpoPsWcCDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "DNk3V3XhcAar"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqu6F9oWcBPy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}